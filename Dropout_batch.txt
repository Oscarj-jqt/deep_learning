# Dropout and Batch Normalization â€” Essential Notes

## 1. Dropout
- Dropout is a technique to prevent overfitting in neural networks.
- During training, dropout randomly "drops" (sets to zero) a fraction of the units in a layer for each batch.
- This forces the network to learn more robust features, not relying too much on any single neuron.
- Typical dropout rates are between 0.2 and 0.5.
- Example:
    ```python
    model = keras.Sequential([
        layers.Dense(16, activation='relu'),
        layers.Dropout(0.3),  # 30% dropout
        layers.Dense(10)
    ])
    ```

## 2. Batch Normalization
- Batch normalization ("batchnorm") normalizes the outputs of a layer to stabilize and speed up training.
- It helps the network train faster and can improve performance, especially on difficult datasets.
- Batch normalization can be added before or after the activation function.
- Example:
    ```python
    model = keras.Sequential([
        layers.Dense(16, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(10)
    ])
    ```

